{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df95286c-5a28-4684-9aa6-d3e27fbb9607",
   "metadata": {},
   "source": [
    "# Workshop 4: Working with multi-dimensional data\n",
    "\n",
    "In this workshop, we provide an introduction to working with multi-dimensional data in Python.\n",
    "\n",
    "**The learning objectives are to:**\n",
    "- provide an overview on multi-dimensional geospatial data using Python \n",
    "- provide basic understanding and examples of working with NetCDF file operations\n",
    "- provide basic understanding and examples of working with CF metadata conventions\n",
    "- provide a brief introduction to CF Python and cf-plot packages\n",
    "- understand the basic data structures in Xarray and how to work with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ee2b2-3ed6-4242-9104-0cf9bab05589",
   "metadata": {},
   "source": [
    "In this workshop (and related HydroJULES workshops) we use Python in Jupyter notebooks. We assume you know the basics of using Python for processing numerical data, and are familiar with Jupyter notebooks. If you would like to improve your confidence and ability with Python and Jupyter notebooks, there is a brief summary below and some learning resources at the end of this notebook. \n",
    "\n",
    "<details>\n",
    "    <summary>NumPy, Pandas, and Matplotlib 101 </summary>\n",
    "\n",
    "In this workshop we are assuming you know about [NumPy](https://numpy.org/) (Numerical Python) and [Pandas](https://pandas.pydata.org/). Numpy is a Python library that provides a multidimensional array object, various derived objects e.g. masked arrays, and an assortment of routines for fast operations on arrays. NumPy arrays are faster and more compact than Python lists. NumPy arrays contain data of the same type. An array is a central data structure of the NumPy library. An array is a grid of values and it contains information about the raw data, how to locate an element, and how to interpret an element. The rank of the array is the number of dimensions. The shape of the array is a tuple of integers giving the size of the array along each dimension.\n",
    "\n",
    "Pandas is a widely used Python library that provides two types of data structure and is often used for data input and output operations. \n",
    "Series: a one-dimensional labeled array holding data of any type e.g. integer.\n",
    "DataFrame: a two-dimensional data structure that holds data like a two-dimension array.\n",
    "\n",
    "[GeoPandas](https://geopandas.org/en/stable/) extends the datatypes used by Pandas to allow spatial operations on geometric types. The core data structure in GeoPandas is the `geopandas.GeoDataFrame`, a subclass of `pandas.DataFrame`, that can store geometry columns and perform spatial operations. The geopandas.GeoSeries, a subclass of pandas.Series, handles the geometries. \n",
    "\n",
    "[Matplotlib](https://matplotlib.org/stable/) is a comprehensive library for creating static, animated, and interactive visualizations. Matplotlib graphs your data on `Figures` (e.g., windows, Jupyter widgets, etc.), each of which can contain one or more `Axes`, an area where points can be specified in terms of x-y coordinates.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc7a9d5-4a5e-4a17-9680-c26c64048ce2",
   "metadata": {},
   "source": [
    "## Multi-dimensional geospatial data 101\n",
    "Multi-dimensional geospatial data refers to datasets that include multiple layers or dimensions of geographical information. This data goes beyond the simple two-dimensional latitude and longitude coordinates and often includes additional dimensions such as time, elevation (altitude), and other variables like temperature, humidity, population density, or any other measurable attribute.\n",
    "\n",
    "This workshop was designed for an hour long session as part of the HydroJULES school. It does not cover all aspects of multi-dimensional data use by the Python scientific communities. Additional resources can be found at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf49cd3-72a6-4c38-a8e3-5127b7f0c63b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Multi-dimensional data and Python packages\n",
    "There are a wide range of Python packages that can be used with multi-dimensional files such as NetCDF files.\n",
    "- In this workshop we focus on [netcdf4-python](https://unidata.github.io/netcdf4-python): is a Python interface to the netCDF C library.\n",
    "- [cf-python](https://ncas-cms.github.io/cf-python/): a Python Earth Science data analysis library that is built on a complete implementation of the CF conventions.\n",
    "- [xarray](https://docs.xarray.dev/en/stable/): makes working with labelled multi-dimensional arrays simple and efficient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d5d05-e283-4a4f-90a0-fedee9a3a981",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary><b>NetCDF and netcdf4-python</b></summary>\n",
    "netcdf4-python is a Python interface to the netCDF C library (Network Common Data Form). It provides a low level interface for working with netCDF datasets in Python. NetCDF refers to a set of software libraries and self-describing, machine-independant data formats that support the creation, access and sharing of array-oriented scientific data. NetCDF was developed in the late 1980s at the Unidata Program Centre, with the objective of building a file format that would permit sharing of data among atmospheric scientists. The stated objectives for the netCDF format were for it to be: self-describing, portable, scalable, appendable, shareable, and archivable. NetCDF4 in ~ 2011 provided the capability to apply attributes to a file as a whole, to a group within a file, or to any individual variable.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95762363-04c9-40f0-82c5-1c754c226768",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2952d5e-0610-4e1f-af35-f7b441a1552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import monthrange\n",
    "import cf\n",
    "import cfplot as cfp\n",
    "from cftime import date2num\n",
    "from datetime import date, datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "from netCDF4 import Dataset, num2date\n",
    "import numpy as np\n",
    "import os as os\n",
    "from pyproj import Proj\n",
    "import time\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "%xmode minimal\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da4648-a8d3-4545-8c86-cbe4c6b648c6",
   "metadata": {},
   "source": [
    "### JASMIN notebook service\n",
    "See how many CPUs and how much RAM is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ed176-3f76-426b-8d33-be8b7d1c04c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CPUs = os.cpu_count()\n",
    "print(\"Total CPUs count:\", CPUs)\n",
    "\n",
    "total_memory, used_memory, free_memory = map(\n",
    "\n",
    "int, os.popen('free -t -m').readlines()[-1].split()[1:])\n",
    "\n",
    "\n",
    "print(\"RAM memory used (%):\", round((used_memory/total_memory) * 100, 1))\n",
    "\n",
    "print(\"RAM total (GB):\", total_memory // 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6bd6b0-ac5e-44e6-ba36-8248d52cf055",
   "metadata": {},
   "source": [
    "### Check what files are in the local data store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3d247-6c9b-44bc-a05c-80be2831c9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Current working directory is:\")\n",
    "print(os.getcwd()+\"\\n\")\n",
    "\n",
    "print(\"This directory contains this notebook and other notebooks in this data lab\")\n",
    "print(str(os.listdir('./'))+\"\\n\")\n",
    "\n",
    "print(\"Root directory contains:\")\n",
    "print(str(os.listdir('/'))+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e510ad-b9fa-4a23-b64b-61fb503cf050",
   "metadata": {},
   "source": [
    "## netCDF\n",
    "### Create gridded data \n",
    "We will generate gridded data to store in netCDF format. Then we will add metadata that follows the Climate and Forecasting conventions. This ensures the dataset can be used by a wide range of tools.\n",
    "\n",
    "Our toy dataset will contain:\n",
    "- Three spatial dimensions (`x`, `y`, and `press`) and one temporal dimension (`times`).\n",
    "- The native coordinate system of the model is on a regular 3km x 3km (`x` and `y`) grid that represents the Earth.\n",
    "- The vertical dimension (`press`) consists of several discrete pressure levels in units of hPa.\n",
    "- The time dimension consists of 12 consecutive hours (`times`), beginning at 2200 UTC on the current day.\n",
    "- A variable of interest (`temps`), which holds the data values at each unique dimensional index. \n",
    "\n",
    "These dimensional arrays are created by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29602bd-4997-4f84-81a1-8253ef33f9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.utcnow().replace(hour=22, minute=0, second=0, microsecond=0)\n",
    "times = np.array([start + timedelta(hours=h) for h in range(13)])\n",
    "\n",
    "x = np.arange(-150, 153, 3)\n",
    "y = np.arange(-100, 100, 3)\n",
    "\n",
    "press = np.array([1000, 925, 850, 700, 500, 300, 250])\n",
    "\n",
    "temps = np.random.randn(times.size, press.size, y.size, x.size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3378ac-f985-4c5f-bf8e-836ac576b9f5",
   "metadata": {},
   "source": [
    "### Creating a new netCDF file\n",
    "First we create a new netCDF file and set up the shared dimensions. We will be using the netcdf4-python package.\n",
    "This file will reside in memory due to the `diskless=True` argument. To create a file on disk, remove it or add the `persist=True` argument.\n",
    "Opening an existing file with `w` as the second argument, will result in existing data being overwritten. If you would like to edit the file, or add to it, open it using `a` as the second argument. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62543e-6cbb-42b0-8028-113861373bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nc = Dataset('example_data.nc', 'w', format='NETCDF4_CLASSIC', diskless=True)\n",
    "\n",
    "# in case file is open, use nc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde678f5-634b-4596-8b18-c257cf237245",
   "metadata": {},
   "source": [
    "### Start setup of this new netCDF file by adding global attribute metadata\n",
    "These are recommended by the CF standard, but are not essential. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa882b8-3898-433b-9ed5-3464bea325f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.Conventions = 'CF-1.11'\n",
    "nc.title = 'Example temperature data'\n",
    "nc.institution = 'UKCEH'\n",
    "nc.source = 'Made up'\n",
    "nc.history = str(datetime.utcnow()) + ' Python'\n",
    "nc.references = ''\n",
    "nc.comment = ''\n",
    "nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63d2482-7b86-4755-a025-5fe25ce56539",
   "metadata": {},
   "source": [
    "### Define dimensions of data, so we can add variables\n",
    "We need to define the dimensions of the data, before we can add any variables to our file. \n",
    "Create dimensions called `x`, `y`, and `pressure`, and set the size of each dimension to the size of the corresponding data array. Set `forecast_time` size to None; this defines the dimension as 'unlimited', so data can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e083d869-edd8-48a5-a628-4300e7af0961",
   "metadata": {},
   "outputs": [],
   "source": [
    "nc.createDimension('forecast_time', None)\n",
    "nc.createDimension('x', x.size)\n",
    "nc.createDimension('y', y.size)\n",
    "nc.createDimension('pressure', press.size)\n",
    "nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b7b74-5538-4b55-a665-f145273e2dcd",
   "metadata": {},
   "source": [
    "### Creating and adding a variable\n",
    "Here we create a `netCDF4 variable` to hold a data field. This includes specifying its data type and dimensions. We can specify if we wish to compress the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7c8bc-6614-449d-b246-e9a894d6d8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_var = nc.createVariable(\n",
    "    'Temperature',\n",
    "    datatype=np.float32,\n",
    "    dimensions=('forecast_time', 'pressure', 'y', 'x'), \n",
    "    zlib=True\n",
    ")\n",
    "nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b0189-2fa6-44b1-9a8e-90a44bf360e9",
   "metadata": {},
   "source": [
    "Next we associate our temperature data with the new variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3324da-6c73-45f5-9231-8963737ec600",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_var[:] = temps\n",
    "temps_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815aff4e-2e6c-4965-9e6c-6fe8f6b62183",
   "metadata": {},
   "source": [
    "### Setting unit attributes\n",
    "CF conventions require a `units` attribute to be set for all variables that represent a dimensional quantity. These enable a user to understand what each variable in a dataset represents.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08dfa9d6-d5f8-4db5-8f2f-394e07f74723",
   "metadata": {},
   "outputs": [],
   "source": [
    "temps_var.units = 'Kelvin'\n",
    "temps_var.standard_name = 'air_temperature'\n",
    "temps_var.long_name = 'Forecast air temperature'\n",
    "temps_var.missing_value = -9999\n",
    "temps_var\n",
    "\n",
    "nc.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ce05ae-f0e9-46ad-8fec-4fbabdecdad0",
   "metadata": {},
   "source": [
    "## CF (Climate and Forecast) conventions\n",
    "[CF Metadata Conventions](https://cfconventions.org/) provide a description of the physical meaning of data and their spatial and temporal properties.\r",
    " They aim:\n",
    "- To enable you to locate data in space-time and as a function of other indpendent variables to facilitate processing and visualisation\n",
    "- Identify data sufficiently to enable users of data from different sources to decide what is comparable, and to distinguish variables in archives.\n",
    "\n",
    " \n",
    "**Principles**\n",
    "- Data should be self-describing.\n",
    "- Metadata readable by humans and computers.\n",
    "- Avoid being too onerous for data-writers and data-readers. \n",
    "ng.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f942b6-2b68-46a6-b9bd-c64d0661a887",
   "metadata": {},
   "source": [
    "<details> \n",
    "    <summary>Further information on their design.</summary>\n",
    "\n",
    "The CF metadata conventions are designed to promote the processing and sharing of files created with the NetCDF API. The conventions define metadata that provide a definitive description of what the data in each variable represents, and the spatial and temporal properties of the data. This enables users of data from different sources to decide which quantities are comparable, and facilitates building applications with powerful extraction, regridding, and display capabilities.\n",
    "\n",
    "The CF data model identifies the fundamental elements (“constructs”) of the CF conventions and shows how they relate to each other, independently of the netCDF coding.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32c2ca2-73e5-4b2c-b1ab-452d927c30d2",
   "metadata": {},
   "source": [
    "## Data for this workshop \n",
    "This workshop is going to explore meteorological driving data from November 2019. The winter of 2019/2020 included periods of very high precipitation, which led to widespread flooding in central and northern England.\n",
    "\n",
    "Here we access the data from the JASMIN Hydro-JULES group workspace. These data are also available from the [Environmental Information Data Centre](https://eidc.ac.uk/) and the [CEDA Archive](https://archive.ceda.ac.uk/). \n",
    "\n",
    "In this workshop we will be using data from the [Climate hydrology and ecology research support system - CHESS](https://catalogue.ceh.ac.uk/documents/7de9790e-66a2-44b5-988e-283d764ef52f).\n",
    "\n",
    "### Floods and Droughts Research Infrastructure\n",
    "In June 2022, the UK Research and Innovation department [announced](https://www.ukri.org/what-we-do/creating-world-class-research-and-innovation-infrastructure/funded-infrastructure-projects/) their intention to invest £38M towards the establishment of a nationwide, digitally enabled Floods and Droughts Research Infrastructure. The core hydrological infrastructure is based around three catchments (100-170 km^2). Further details are available in the UKCEH [FDRI scoping project: community report](https://www.ceh.ac.uk/our-science/projects/floods-and-droughts-research-infrastructure-fdri-scoping-report).\n",
    "\n",
    "**National River Flow Archive stations**\n",
    "- [Tweed at Kingledores](https://nrfa.ceh.ac.uk/data/station/info/21014)\n",
    "- [Severn at Dolwen](https://nrfa.ceh.ac.uk/data/station/info/54080)\n",
    "- [Chess at Rickmansworth](https://nrfa.ceh.ac.uk/data/station/info/39088)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f8893e-c079-4fdd-a924-5074bd685103",
   "metadata": {},
   "source": [
    "### 1km daily precipitation data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee2e48e-37e3-4505-906e-49f26665504c",
   "metadata": {},
   "source": [
    "The following data used for training is stored in JASMIN GroupWorkspace (GWS) and is accessed through it for anyone with JASMIN access. However, to work with this data beyond the JASMIN platform please download it from [EIDC link](https://doi.org/10.5285/835a50df-e74f-4bfb-b593-804fd61d5eab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082a2ab-bf4c-4a17-9271-51b2b1908f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a month's worth of CHESS precipitation data from a local file\n",
    "# With JASMIN gws path\n",
    "\n",
    "path = '/gws/nopw/j04/hydro_jules/data/uk/driving_data/chess/chess-met/daily/'\n",
    "file = 'chess-met_precip_gb_1km_daily_20191101-20191130.nc'\n",
    "\n",
    "# If you are not working on JASMIN platform, please download this data at\n",
    "# https://doi.org/10.5285/835a50df-e74f-4bfb-b593-804fd61d5eab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24e0363-ce8c-416b-be5b-f376b63ffa77",
   "metadata": {},
   "source": [
    "### Locations of the three FDRI catchment outlets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c4325-c9c2-4559-bdc1-f71c3a822cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the netCDF file\n",
    "dataset = Dataset(path+file)\n",
    "\n",
    "precipDataS = dataset.variables['precip'][:,528,411] # kg/m2/s\n",
    "precipDataD = precipDataS*86400 # mm/day\n",
    "\n",
    "# Day 7\n",
    "precipMapST0 = dataset.variables['precip'][7,:,:]\n",
    "precipMapDT0 =  precipMapST0*86400 \n",
    "\n",
    "# Specify the locations (lat, lon)\n",
    "locations = [\n",
    "    (528, 411),  # 21014 - Tweed at Kingledores\n",
    "    (285, 299),  # 54080 - Severn at Dolwen\n",
    "    (195, 506)   # 39088 - Chess at Rickmansworth\n",
    "]\n",
    "\n",
    "# Extract the coordinates and labels\n",
    "latitudes, longitudes = zip(*locations)\n",
    "labels = ['21014 - Tweed at Kingledores', '54080 - Severn at Dolwen', '39088 - Chess at Rickmansworth']\n",
    "\n",
    "# Plot the map of the UK and the locations\n",
    "plt.figure(figsize=(6, 10))\n",
    "plt.imshow(precipMapDT0, origin='lower')\n",
    "plt.scatter(longitudes, latitudes, color='red', zorder=5)\n",
    "for i, label in enumerate(labels):\n",
    "    plt.text(longitudes[i] + 5, latitudes[i] + 5, label, fontsize=12, color='black', zorder=10)\n",
    "\n",
    "plt.title('Locations in the UK')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "# plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1199e065-2e4f-4677-8120-f64170d57bed",
   "metadata": {},
   "source": [
    "### Plotting the precipitation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c5833-e7d8-4a25-980a-e2c704f04de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the precipitation variable\n",
    "precip = dataset.variables['precip']  # Replace 'precipitation' with the actual variable name\n",
    "\n",
    "# old\n",
    "# Initialize lists to store time and precipitation data for each location\n",
    "time = dataset.variables['time'][:]  # Adjust as necessary to get the time variable\n",
    "precip_data = []\n",
    "\n",
    "# Loop over each location to extract data\n",
    "for loc in locations:\n",
    "    lat, lon = loc\n",
    "    # Convert from kg/m²/s to mm/day by multiplying by 86400\n",
    "    precip_data.append(precip[:, lat, lon] * 86400)\n",
    "\n",
    "# Plotting the data\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Todo fix label\n",
    "# latitudes, longitudes = zip(*locations)\n",
    "# labels = ['21014 - Tweed at Kingledores', '54080 - Severn at Dolwen', '39088 - Chess at Rickmansworth']\n",
    "# for i, label in enumerate(labels):\n",
    "#     plt.text(longitudes[i] + 5, latitudes[i] + 5, label, fontsize=12, color='black', zorder=10)\n",
    "\n",
    "for i, loc in enumerate(locations):\n",
    "    plt.scatter(time, precip_data[i], label=f'Location {loc}',)\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Precipitation')\n",
    "plt.title('Precipitation Data for Multiple Locations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Close the dataset\n",
    "dataset.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e13213-58c1-4a15-9701-b97d1ca5ce99",
   "metadata": {},
   "source": [
    "### Explore the variables in a netCDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8df4f-9ac8-4c03-8aa2-73630e003ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write out the variables in a netCDF file\n",
    "my_dataset = Dataset(path+file)\n",
    "\n",
    "print(\"Loop through variables and print variable name:\")\n",
    "for v in my_dataset.variables:\n",
    "    print(v)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Select one variable by its name and print one metadata attibute ('units'), \\\n",
    " and the dimensions and shape of the variable:\")\n",
    "\n",
    "print(\"Units : \"+my_dataset.variables['precip'].units)\n",
    "print(\"Dimensions : \"+str(my_dataset.variables['precip'].dimensions))\n",
    "print(\"Shape : \"+str(my_dataset.variables['precip'].shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Print full metadata for all variables:\")\n",
    "print(\"\\n\")\n",
    "print(my_dataset.variables)\n",
    "\n",
    "my_dataset.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a807cd-4793-444a-8138-1046127195dc",
   "metadata": {},
   "source": [
    "### A closer look at the precipitation variable and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae4474-2f7f-4a01-a43a-537308b8fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset = Dataset(path+file)\n",
    "\n",
    "print(\"First get the precipitation 'precip' variable, and show its dimensions\")\n",
    "precip_var = my_dataset.variables['precip']\n",
    "print(\"Dimensions : \"+str(precip_var.dimensions))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Then get the precipitation from this variable and check its type and shape:\")\n",
    "precip=precip_var[:]\n",
    "print(\"Type : \"+str(type(precip)))\n",
    "print(\"Shape : \"+str(precip.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"It's a 3D masked array (array of data values, plus another array of mask values) \\\n",
    "with a grid of X-Y values for each day of June 2019\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"First, look at the mask\")\n",
    "print(\"It has the same shape as the values:\")\n",
    "precip_mask = precip.mask\n",
    "print(\"Type : \"+str(type(precip_mask)))\n",
    "print(\"Shape : \"+str(precip_mask.shape))\n",
    "print(\"Unique values : \"+str(np.unique(precip_mask)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Make a quick plot of the mask for the first time step:\")\n",
    "plt.imshow(precip_mask[0,:,:], origin='lower')\n",
    "\n",
    "my_dataset.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b734d8-8fe4-4e6d-ab9a-13702a8bc9c1",
   "metadata": {},
   "source": [
    "### Explore the spatial variables: latitude/longitude, and X/Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be8a8b-f887-4ed7-a9ad-1ec536aa2d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the lat long of the grid cell at x = 400, y=500\n",
    "# Amulya code - maybe reduce in half\n",
    "my_dataset = Dataset(path+file)\n",
    "\n",
    "print(\"First get the x and y variables and show its dimensions and shape\")\n",
    "x_var = my_dataset.variables['x']\n",
    "print(\"Dimensions : \"+str(x_var.dimensions))\n",
    "print(\"Shape : \"+str(x_var[:].shape))\n",
    "y_var = my_dataset.variables['y']\n",
    "print(\"Dimensions : \"+str(y_var.dimensions))\n",
    "print(\"Shape : \"+str(y_var[:].shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"They are 1D arrays of the British National Grid easting and northing coordinate values\")\n",
    "print(\"of the centre points of the data cells at the equivalent grid cells in the data array\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"For example, the easting and northing of the value at index 500,400 is:\")\n",
    "print(\"Easting : \"+str(x_var[500]))\n",
    "print(\"Northing : \"+str(y_var[400]))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"There is also a variable containing the minimum and maximum easting and northing for each grid cell\")\n",
    "x_bnd_var = my_dataset.variables['x_bnds']\n",
    "print(\"Dimensions : \"+str(x_bnd_var.dimensions))\n",
    "print(\"Shape : \"+str(x_bnd_var[:].shape))\n",
    "y_bnd_var = my_dataset.variables['y_bnds']\n",
    "print(\"Dimensions : \"+str(y_bnd_var.dimensions))\n",
    "print(\"Shape : \"+str(y_bnd_var[:].shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The boundaries of the cell at index 500,400 is:\")\n",
    "print(\"Easting : \"+str(x_bnd_var[500,:]))\n",
    "print(\"Northing : \"+str(y_bnd_var[400,:]))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Next get the lat variable, and show its dimensions\")\n",
    "lat_var = my_dataset.variables['lat']\n",
    "print(\"Dimensions : \"+str(lat_var.dimensions))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Then get the latitude from this variable and check its type and shape:\")\n",
    "lat=lat_var[:]\n",
    "print(\"Type : \"+str(type(lat)))\n",
    "print(\"Shape : \"+str(lat.shape))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"It's a 2D masked array (array of values. plus another array of mask values)\")\n",
    "print(\"The dimensions are X and Y : there is a different latitude values for each combination of X and Y\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"But look at the shape of the mask - it's different from the values:\")\n",
    "lat_mask = lat.mask\n",
    "print(\"lat mask type : \"+str(type(lat_mask)))\n",
    "print(\"lat mask shape : \"+str(lat_mask.shape))\n",
    "print(\"lat mask : \"+str(lat_mask))\n",
    "print(\"\\n\")\n",
    "\n",
    "lat_data=lat.data\n",
    "print(\"lat data type : \"+str(type(lat_data)))\n",
    "print(\"lat data shape : \"+str(lat_data.shape))\n",
    "print(\"\\n\")\n",
    "print(\"The mask is just a single value 'False'. \\\n",
    "netCDF4 variables will be masked arrays, but we cannot expect the mask to be the same size as the data.\")\n",
    "print(\"Though often, particularly in Hydro-JULES datasets, the data variables will have a mask to enable \\\n",
    "the points with data (land points) to be readily identified\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"The data is a numpy array, and we can simply select the value at the indices of interest.\")\n",
    "grid_lat = lat_data[500,400]\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the longitude directly in one line\n",
    "grid_lon = my_dataset.variables['lon'][:].data[500,400]\n",
    "\n",
    "print(\"Lat and Long at y=500, x=400 is:\")\n",
    "print([grid_lat,grid_lon])\n",
    "\n",
    "my_dataset.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5f4aab-37b1-49e7-87a2-bc22619a1798",
   "metadata": {},
   "source": [
    "### Find the nearest cell for a given easting and northing, or latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c837fa-639e-4cb1-9b80-3a81c81cdf0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "easting = 529010\n",
    "northing = 180220\n",
    "\n",
    "my_dataset = Dataset(path+file)\n",
    "\n",
    "# Get the easting and northing variables from the dataset\n",
    "x = my_dataset.variables['x'][:]\n",
    "y = my_dataset.variables['y'][:]\n",
    "\n",
    "# Get the index of the closest value to the easting and northing (the index of the cell we want)\n",
    "idx_e = (np.abs(x - easting)).argmin()\n",
    "idx_n = (np.abs(y - northing)).argmin()\n",
    "print('Original easting and northing : {}, {} \\n'.format( *[ easting, northing ] ))\n",
    "print('Nearest cell at easting {} , northing {} within index [{},{}] \\n'.format( *[ x[idx_e] , y[idx_n] , idx_e, idx_n ]))\n",
    "\n",
    "my_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19903316-8fb6-409f-b4c1-64c1841dd718",
   "metadata": {},
   "source": [
    "The lat and long variables are 2 dimensional arrays of the value at each cell (defined by easting and northing) This means identifying the nearest cell for a given lat and long is not so simple (it's not just the cell with the nearest latitude and the cell with the nearest longitude). This code quickly identifies the X and Y indices for the nearest cell for a given lat and long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6041f66-0f37-4343-be1b-65c2cbdf17a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Latitude and longitude of interest:\n",
    "my_lat = 54.39\n",
    "my_lon = -1.99\n",
    "\n",
    "# Open the dataset\n",
    "my_dataset = Dataset(path+file)\n",
    "# retrieve the lat and long arrays\n",
    "lat = my_dataset.variables['lat'][:]\n",
    "lon = my_dataset.variables['lon'][:]\n",
    "# Create new arrays of the distance for each cell from the given latitude and the given longitude\n",
    "lat_dist = abs(lat-my_lat)\n",
    "lon_dist = abs(lon-my_lon)\n",
    "# Create an array of the absolute distance (in degrees)\n",
    "dist_diff = np.sqrt( np.square(lat_dist) + np.square(lon_dist) )\n",
    "# Find the cell where this distance is a minimum\n",
    "idx = np.where( dist_diff == np.min(dist_diff) )\n",
    "\n",
    "my_x = idx[0][0]\n",
    "my_y = idx[1][0]\n",
    "\n",
    "# Get the precipitation data for this location (convert from kg/m²/s to mm/day) \n",
    "precip = my_dataset.variables['precip'][ :, my_y, my_x ] * 86400\n",
    "\n",
    "my_dataset.close()\n",
    "\n",
    "print(\"Nearest grid cell is : \"+str(idx))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Show the grids around this point\n",
    "grid_size=2\n",
    "minx = my_x-grid_size\n",
    "maxx = my_x+grid_size+1\n",
    "miny = my_y-grid_size\n",
    "maxy = my_y+grid_size+1\n",
    "\n",
    "with np.printoptions(precision=7):\n",
    "    print(\"Latitude values nearby\")\n",
    "    print(lat[minx:maxx,miny:maxy])\n",
    "    print(\"\\n\")\n",
    "    print(\"Longitude values nearby\")\n",
    "    print(lon[minx:maxx,miny:maxy])\n",
    "    print(\"\\n\")\n",
    "    print(\"Distances from given lat/long nearby \")\n",
    "    print(np.round(dist_diff[minx:maxx,miny:maxy],5))\n",
    "    print(\"\\n\")\n",
    "    print(\"Minimum distance:\")\n",
    "    print (np.min(dist_diff))\n",
    "    print(\"\\n\")\n",
    "print(\"Data for point [{},{}]\".format( *[ my_x, my_y ]))\n",
    "# plt.plot(precip, 'bo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91eb2a-962b-491c-9124-163b84646533",
   "metadata": {},
   "source": [
    "### Explore the time variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9166c-e300-401b-885f-d3a5cbc62e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the time variable to identify the correct indices for dates of interest\n",
    "start_date = date ( year=2019, month=11, day=1)        # 01/11/2019\n",
    "end_date = date ( year=2015, month=11, day=30)         # 30/11/2019\n",
    "\n",
    "var = 'precip'                                            # Precipitation\n",
    "\n",
    "# Open the dataset\n",
    "my_dataset = Dataset(path+file)\n",
    "\n",
    "# Get the time variable from the dataset\n",
    "print('Time variable units:')\n",
    "print(my_dataset.variables['time'].units+':\\n')\n",
    "my_dates = my_dataset.variables['time'][:]\n",
    "\n",
    "# Get the index of the start and end date \n",
    "# (the array \"dates\" is a list of the dates expressed as the number of days since 1961)\n",
    "s_days = ( start_date - date( year=1961, month=1, day=1 ) ).days\n",
    "idx_start = (np.abs(my_dates - s_days)).argmin()\n",
    "e_days = ( end_date - date( year=1961, month=1, day=1 ) ).days\n",
    "idx_end = (np.abs(my_dates - e_days)).argmin()\n",
    "\n",
    "# Get the data (air temp) variable\n",
    "precip = my_dataset.variables[var]\n",
    "\n",
    "# Write out the metadata\n",
    "print('Precipitation variable information:')\n",
    "print(precip)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the data between the start and end date index, at the easting and northing indices calculated earlier\n",
    "ts_data = precip[ idx_start:idx_end, my_y, my_x ]\n",
    "\n",
    "# Close the dataset\n",
    "my_dataset.close()\n",
    "\n",
    "# Todo\n",
    "\n",
    "print(\"Data for index {} to {} :\".format( *[ idx_start, idx_end ] ))\n",
    "print(ts_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8a7a11-9633-4ef0-84c7-cb947c2c5b97",
   "metadata": {},
   "source": [
    "## Read a netCDF dataset over the web\n",
    "netCDF files are commonly made available over the web using a THREDDS server. Datasets made accessible in this way can be readily queried using exactly the same code as described above, but reading from a THREDDS API end point instead of a file.\n",
    "\n",
    "This example reads air temperature data from the CHESS dataset hosted bu UKCEH's EIDC. In this case the entire dataset is available, but the web-based access is considerably slower than file access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25d2ae-57ef-4e91-8b85-39c98447cee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of a file, we are now going to access data over the web\n",
    "# The interface to the dataset, made accessible through a THREDDS server, is at:\n",
    "url = \"https://eip.ceh.ac.uk/thredds/dodsC/public-chess/driving_data/aggregation/precip_aggregation\"\n",
    "\n",
    "# We access the netCDF dataset object in the same way as before\n",
    "my_dataset = Dataset(url)\n",
    "\n",
    "for v in my_dataset.variables:\n",
    "    print(v)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(my_dataset)\n",
    "print(\"\\n\")\n",
    "print(my_dataset.dimensions)\n",
    "print(\"\\n\")\n",
    "print(my_dataset.variables)\n",
    "print(\"\\n\")\n",
    "\n",
    "print(my_dataset.variables[\"precip\"])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Get the time variable from the dataset\n",
    "print(\"Time variable units:\")\n",
    "print(my_dataset.variables[\"time\"].units+\":\\n\")\n",
    "\n",
    "# Whole time series from this aggregation is >50 years, so select an arbitrary subset for an arbitrary cell\n",
    "# my_dates = my_dataset.variables[\"time\"][500:530]\n",
    "ts_data = my_dataset.variables[\"precip\"][500:530,400,500]\n",
    "\n",
    "# Todo\n",
    "\n",
    "plt.plot(ts_data, 'bo')\n",
    "\n",
    "\n",
    "my_dataset.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0d739-78dc-43e2-8ab1-ec03fcec8caa",
   "metadata": {},
   "source": [
    "## cf Python and cf-plot\n",
    "\n",
    "The Python [cf package](https://ncas-cms.github.io/cf-python/index.html) is an Earth Science data analysis library that is built on a complete implementation of the CF data model.\n",
    "It can be used to read and write NetCDF files. \n",
    "\n",
    "[cf-plot](https://ajheaps.github.io/cf-plot/) is a set of Python routines for making the common contour, vector and line plots that climate researchers use.\n",
    "These packages only work on Linux OS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98af7c4d-8c0d-4bac-a45c-3feaad12feaa",
   "metadata": {},
   "source": [
    "## Xarray\n",
    "\n",
    "Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like multidimensional arrays, which allows for a more intuitive, more concise, and less error-prone developer experience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aafbda-f717-4a19-b12e-2c6c833bb8b2",
   "metadata": {},
   "source": [
    "What labels enable\n",
    "Datasets often have labels which encode information about how array values map to locations in space, time, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1025c0a-89bb-45da-8357-ff15716b39d4",
   "metadata": {},
   "source": [
    "### Core data structures\n",
    "Xarray has two core data structures (both are fundamentally N-dimensional):\n",
    "`DataArray` is a labeled, N-dimensional array.\n",
    "`Dataset` is a multi-dimensional, in-memory array database. It is a dict-like container of `DataArray` objects aligned along any number of shared dimensions, and serves a similar purpose in xarray to the pandas.DataFrame. \n",
    "The power of the dataset over a plain dictionary is that, in addition to pulling out arrays by name, it is possible to select or combine data along a dimension across all arrays simultaneously.\n",
    "The data model is borrowed from netCDF file format, which also provides xarray with a natural and portable serialization format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2ff2a-b226-4810-9a3f-803d361e59b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a month's worth of CHESS precipitation data from a local file\n",
    "# With JASMIN gws path\n",
    "path = '/gws/nopw/j04/hydro_jules/data/uk/driving_data/chess/chess-met/daily/'\n",
    "file = 'chess-met_precip_gb_1km_daily_20191101-20191130.nc'\n",
    "\n",
    "# If you are not working on JASMIN platform, please download this data at\n",
    "# https://doi.org/10.5285/835a50df-e74f-4bfb-b593-804fd61d5eab\n",
    "\n",
    "# Open the netCDF file\n",
    "ds = xr.open_dataset(path+file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0bc67cf-48ca-4e1f-9370-9044384ee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972f2683-c024-448a-ae24-48cd164e173e",
   "metadata": {},
   "source": [
    "### Exploring a Dataset and its DataArrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c5361d-41c2-44e0-9686-16a48e39d020",
   "metadata": {},
   "source": [
    "Datasets are dictionary-like containers of “DataArray”s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0845819e-f737-4fcf-bd3c-84d9d5cda6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use dictionary syntax to extract dataarray\n",
    "ds[\"precip\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a8817b-cc3e-4020-bcd3-1bfd15056229",
   "metadata": {},
   "outputs": [],
   "source": [
    " #use dot notation to extract dataarray\n",
    "ds.precip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc220fe3-fa9a-406f-a4ee-2c25bda78976",
   "metadata": {},
   "source": [
    "A DataArray contains data and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b1db5-1b57-4801-a3ec-4adba466bcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "da = ds.precip\n",
    "\n",
    "da.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ad3017-600b-4e91-abd1-f7346ec21103",
   "metadata": {},
   "source": [
    "Named dimensions\n",
    ".dims correspond to the axes of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668e7c5-88aa-485a-909e-2b4ce8fe9013",
   "metadata": {},
   "outputs": [],
   "source": [
    "da.dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4594d0-153d-4a45-bd5e-437d8ebc9105",
   "metadata": {},
   "source": [
    "Coordinate variables\n",
    ".coords is a data container for coordinate variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658d68d-016f-4003-a32e-806eccd18798",
   "metadata": {},
   "source": [
    "Advantages of Xarray\n",
    "Xarray supports using dimension names instead of axis numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6250e20-4138-4693-b117-a5f185ef1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.precip.mean(dim=\"time\").plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64baec8c-6f97-4667-9c67-0f3fb55313b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xarray supports label based positioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd97a9b9-187c-4ef3-aa3a-ce50efba0bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.sel(time=\"2019-11-7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65095e54-4388-4c5c-8aff-3bab39d426af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demonstrate slicing\n",
    "ds.sel(time=slice(\"2019-11-01\", \"2019-11-07\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36a85bd-e3ee-476d-9865-589c6d875a89",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "### Python and Jupyter notebooks\n",
    "\n",
    "JupyterLab Documentation — JupyterLab 4.2.2 documentation\n",
    "https://jupyterlab.readthedocs.io/en/stable/\n",
    "\n",
    "ceda-notebooks/notebooks/training/intro at master · cedadev/ceda-notebooks\n",
    "https://github.com/cedadev/ceda-notebooks/tree/master/notebooks/training/intro\n",
    "\n",
    "### JASMIN\n",
    "\n",
    "ceda-notebooks/notebooks/training/intro/notebook-tour.ipynb at master · cedadev/ceda-notebooks\n",
    "https://github.com/cedadev/ceda-notebooks/blob/master/notebooks/training/intro/notebook-tour.ipynb\n",
    "\n",
    "cedadev/jasmin-workshop: Materials and code for JASMIN workshops\n",
    "https://github.com/cedadev/jasmin-workshop\n",
    "\n",
    "#### Python\n",
    "Scientific Python Lectures — Scientific Python Lectures\n",
    "https://lectures.scientific-python.org/\n",
    "\n",
    "### Packages\n",
    "\n",
    "\n",
    "#### NumPy\n",
    "NumPy: the absolute basics for beginners — NumPy v2.0 Manual\n",
    "https://numpy.org/doc/stable/user/absolute_beginners.html\n",
    "\n",
    "#### NetCDF\n",
    "https://unidata.github.io/netcdf4-python/\n",
    "\n",
    "#### CF\n",
    "https://ncas-cms.github.io/cf-python/index.html\n",
    "\n",
    "Visualisation\n",
    "https://ajheaps.github.io/cf-plot/user_guide.html\n",
    "\n",
    "#### Xarray\n",
    "Xarray documentation\n",
    "https://docs.xarray.dev/en/stable/index.html\n",
    "\n",
    "Terminology\n",
    "https://docs.xarray.dev/en/stable/user-guide/terminology.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c748cb4e-719e-4fa0-b0e7-8425870f02c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
